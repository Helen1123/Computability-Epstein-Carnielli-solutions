The following questions refer to the article by Gina Bari Kolata in G.1 above.
a.What is the difference between an error in a proof done by a computer and an error in a proof written by a mathematician? Why
do we sometimes accept proofs in mathematics even though they may have "trivial" errors?

b. What is wrong with the following statement as a description of the method of testing for primes:"In many cases it may be 
possible to "prove" statements with the aid of a computer if the computer is allowed to  err with the predetermined low 
probability"?

c. Is the following assertiom accurate: "The glory of mathematics is that existing methods of proof are essentially error-free." 

a.The difference is that computer is programmed to make mistakes deliberately, to speed up the process of proving, while a 
mathematician did not intend to make the mistake. We accept such "erroneous" proofs, because it enables us to get the result 
faster (like in the example of finding prime numbers).

b. Probably, the word in many cases. It is true for almost all cases. And it is already possible not may be possible. 

c. I think it is not accurate, as the main problem is that they are error-free, but that they either do not exist or take an
enormous amount of time to perform.  
